# Lab â€“ Task 2: ImplementaciÃ³n â€œBare Metalâ€ de MÃ©todos de Gradiente

## ğŸ“Œ DescripciÃ³n

Este laboratorio tiene como objetivo implementar manualmente tres variantes del algoritmo de descenso por gradiente para ajustar un polinomio de grado 3 a un conjunto de datos ruidoso.

Se comparan:

- Batch Gradient Descent
- Stochastic Gradient Descent (SGD)
- Mini-batch Gradient Descent

El enfoque es completamente **â€œbare metalâ€**, es decir:

âš ï¸ No se permite usar:
- sklearn
- pytorch
- tensorflow

Solo se permite:
- numpy
- pandas
- matplotlib

Toda la lÃ³gica matemÃ¡tica y las actualizaciones de los pesos fueron implementadas manualmente.

---

## ğŸ¯ Objetivo del Experimento

Visualizar la diferencia entre:

- Velocidad de convergencia
- Estabilidad
- Error final alcanzado

utilizando como modelo un polinomio cÃºbico.

---

## ğŸ“Š GeneraciÃ³n de Datos

Se generaron 1000 puntos basados en la funciÃ³n real:

y = 2xÂ³ âˆ’ 3xÂ² + 5x + 3

Posteriormente:

- Se agregÃ³ ruido normal aleatorio.
- Se normalizaron los datos para mejorar la estabilidad del gradiente.
- Se construyÃ³ la matriz de diseÃ±o:

X = [1, x, xÂ², xÂ³]

---

## ğŸ§® Modelo MatemÃ¡tico

Modelo:

Å· = Xw

FunciÃ³n de costo (MSE):

J(w) = (1/n) âˆ‘ (Å· âˆ’ y)Â²

Gradiente vectorial:

âˆ‡J = (2/n) Xáµ€ (Xw âˆ’ y)

ActualizaciÃ³n de pesos:

w = w âˆ’ Î· âˆ‡J

---

## âš™ï¸ Algoritmos Implementados

### 1ï¸âƒ£ Batch Gradient Descent

- Utiliza todos los datos en cada actualizaciÃ³n.
- Gradiente exacto.
- Convergencia estable.

---

### 2ï¸âƒ£ Stochastic Gradient Descent (SGD)

- Utiliza un solo punto por actualizaciÃ³n.
- Gradiente altamente ruidoso.
- Mayor variabilidad.
- Oscila alrededor del mÃ­nimo.

**Importante:**

SGD presenta alta variabilidad debido a que usa un solo punto por actualizaciÃ³n, lo que genera un gradiente ruidoso y provoca fluctuaciones alrededor del mÃ­nimo.

---

### 3ï¸âƒ£ Mini-batch Gradient Descent

- Utiliza lotes de tamaÃ±o 32.
- Compromiso entre estabilidad y eficiencia.
- Menor varianza que SGD.
- Menor costo por actualizaciÃ³n que Batch.

---

## ğŸ“ˆ MÃ©trica de EvaluaciÃ³n

Se graficÃ³:

Loss (MSE) vs Tiempo (segundos)

âš ï¸ No se usaron iteraciones en el eje X, sino tiempo real, para comparar eficiencia computacional.

---

## ğŸ” Resultados Observados

- **Batch GD**
  - Convergencia suave y estable.
  - Error final mÃ¡s bajo.
  - Mayor costo por actualizaciÃ³n.

- **SGD**
  - Convergencia rÃ¡pida al inicio.
  - Alta variabilidad.
  - Oscilaciones constantes alrededor del mÃ­nimo.
  - Error final ligeramente mayor.

- **Mini-batch**
  - Balance entre estabilidad y velocidad.
  - Convergencia relativamente rÃ¡pida.
  - Error final cercano al de Batch.

---

## ğŸ§  ConclusiÃ³n

El experimento demuestra la relaciÃ³n entre varianza del gradiente y estabilidad:

- Batch GD tiene baja varianza â†’ convergencia estable.
- SGD tiene alta varianza â†’ fluctuaciones alrededor del mÃ­nimo.
- Mini-batch logra un equilibrio entre ambos.

En tÃ©rminos de estabilidad y error final, Batch Gradient Descent obtiene el resultado mÃ¡s consistente.

En tÃ©rminos de velocidad inicial de convergencia, SGD reduce rÃ¡pidamente el error, pero su naturaleza ruidosa impide una estabilizaciÃ³n completa.

Mini-batch representa el mejor compromiso prÃ¡ctico.

---

## ğŸ“‚ Estructura del Proyecto

```bash
â”‚
â”œâ”€â”€ task2_1.py
â”œâ”€â”€ README.md
â””â”€â”€ resultados.png
```

## ğŸš€ CÃ³mo Ejecutar

```bash
python task2_1.py
```

## ğŸ“Œ Aprendizajes Clave

La normalizaciÃ³n es crÃ­tica para estabilidad numÃ©rica.

La varianza del estimador del gradiente afecta directamente la convergencia.

SGD no converge suavemente, sino que oscila alrededor del mÃ­nimo.

Mini-batch es el mÃ©todo mÃ¡s utilizado en prÃ¡ctica debido a su balance.

## ğŸ‘©â€ğŸ’»ğŸ‘©â€ğŸ’» Autoras
Camila Richter - 23183
MarinÃ©s GarcÃ­a - 23391
